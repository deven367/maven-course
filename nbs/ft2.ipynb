{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Workshop 2\n",
    "\n",
    "> Fine-Tuning with Axolotl (guest speakers Wing Lian, Zach Mueller)\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Full fine-tune vs LoRA\n",
    "   1. At this point of time, full fine-tune is rarely used. LoRA is preferred as it significantly reduces the number of parameters to be trained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-tip}\n",
    "## Research thought\n",
    "\n",
    "Can we have an adapter and we can somehow find ways to evaluate that adapter from a perspective of free recall?\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
